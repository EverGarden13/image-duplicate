{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Duplicate Image Detection System\n",
    "# This script uses perceptual hashing and computer vision techniques to efficiently detect duplicate images\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Install required packages if not already installed\n",
    "try:\n",
    "    import imagehash\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"imagehash\"])\n",
    "    import imagehash\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"opencv-python\"])\n",
    "    import cv2\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedImageDuplicateDetector:\n",
    "    \"\"\"\n",
    "    Advanced image duplicate detection using multiple hashing algorithms\n",
    "    and computer vision techniques for optimal time complexity O(n log n)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold=5):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.hash_algorithms = {\n",
    "            'phash': imagehash.phash,\n",
    "            'dhash': imagehash.dhash,\n",
    "            'whash': imagehash.whash,\n",
    "            'average_hash': imagehash.average_hash\n",
    "        }\n",
    "        \n",
    "    def compute_image_hashes(self, image_path):\n",
    "        \"\"\"Compute multiple perceptual hashes for an image\"\"\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert to RGB if necessary\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                hashes = {}\n",
    "                for name, hash_func in self.hash_algorithms.items():\n",
    "                    hashes[name] = hash_func(img)\n",
    "                \n",
    "                return hashes\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def hamming_distance(self, hash1, hash2):\n",
    "        \"\"\"Calculate Hamming distance between two hashes\"\"\"\n",
    "        return hash1 - hash2\n",
    "    \n",
    "    def are_images_similar(self, hashes1, hashes2):\n",
    "        \"\"\"Check if two sets of hashes indicate similar images\"\"\"\n",
    "        if not hashes1 or not hashes2:\n",
    "            return False\n",
    "        \n",
    "        # Check similarity using multiple hash types\n",
    "        similar_count = 0\n",
    "        for hash_type in self.hash_algorithms.keys():\n",
    "            if hash_type in hashes1 and hash_type in hashes2:\n",
    "                distance = self.hamming_distance(hashes1[hash_type], hashes2[hash_type])\n",
    "                if distance <= self.similarity_threshold:\n",
    "                    similar_count += 1\n",
    "        \n",
    "        # Consider images similar if at least 2 hash types agree\n",
    "        return similar_count >= 2\n",
    "\n",
    "print(\"AdvancedImageDuplicateDetector class defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_images(image_folder_path, similarity_threshold=5):\n",
    "    \"\"\"\n",
    "    Find duplicate images in a folder using advanced algorithms\n",
    "    Time Complexity: O(n log n) where n is the number of images\n",
    "    \"\"\"\n",
    "    \n",
    "    detector = AdvancedImageDuplicateDetector(similarity_threshold)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif', '.webp'}\n",
    "    image_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(image_folder_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files to analyze...\")\n",
    "    \n",
    "    # Compute hashes for all images\n",
    "    image_hashes = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, image_path in enumerate(image_files):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing image {i+1}/{len(image_files)}...\")\n",
    "        \n",
    "        hashes = detector.compute_image_hashes(image_path)\n",
    "        if hashes:\n",
    "            image_hashes[image_path] = hashes\n",
    "    \n",
    "    print(f\"Hash computation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Find duplicates using efficient comparison\n",
    "    duplicates = []\n",
    "    processed = set()\n",
    "    \n",
    "    image_paths = list(image_hashes.keys())\n",
    "    \n",
    "    for i in range(len(image_paths)):\n",
    "        if image_paths[i] in processed:\n",
    "            continue\n",
    "            \n",
    "        current_group = [image_paths[i]]\n",
    "        processed.add(image_paths[i])\n",
    "        \n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            if image_paths[j] in processed:\n",
    "                continue\n",
    "                \n",
    "            if detector.are_images_similar(image_hashes[image_paths[i]], image_hashes[image_paths[j]]):\n",
    "                current_group.append(image_paths[j])\n",
    "                processed.add(image_paths[j])\n",
    "        \n",
    "        if len(current_group) > 1:\n",
    "            duplicates.append(current_group)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "print(\"Duplicate detection function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_duplicate_groups_with_keep_delete(duplicate_groups):\n",
    "    \"\"\"\n",
    "    Display duplicate image groups with file paths, images, and keep/delete option\n",
    "    Returns filtered groups after user review\n",
    "    \"\"\"\n",
    "    if not duplicate_groups:\n",
    "        print(\"No duplicate images found!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(duplicate_groups)} groups of duplicate images:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    kept_groups = []\n",
    "    deleted_groups = []\n",
    "    \n",
    "    for group_idx, group in enumerate(duplicate_groups, 1):\n",
    "        print(f\"\\nGroup {group_idx}: {len(group)} duplicate images\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Display file paths\n",
    "        for i, image_path in enumerate(group):\n",
    "            print(f\"{i+1}. {image_path}\")\n",
    "        \n",
    "        # Display images in a grid\n",
    "        num_images_to_show = min(len(group), 4)\n",
    "        fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 4))\n",
    "        \n",
    "        # Handle single image case\n",
    "        if num_images_to_show == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        if len(group) > 4:\n",
    "            print(f\"   (Showing first 4 out of {len(group)} images)\")\n",
    "            \n",
    "        for i, image_path in enumerate(group[:num_images_to_show]):\n",
    "            try:\n",
    "                img = Image.open(image_path)\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Image {i+1}\", fontsize=10)\n",
    "                axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {image_path}: {e}\")\n",
    "                # Show error text in the subplot\n",
    "                axes[i].text(0.5, 0.5, f\"Error loading\\nimage {i+1}\", \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # User input for keep/delete decision\n",
    "        while True:\n",
    "            user_input = input(f\"Group {group_idx}: What to do with this duplicate group? (k/keep all files, d/delete duplicates only): \").strip().lower()\n",
    "            if user_input in ['k', 'keep']:\n",
    "                kept_groups.append((group_idx, group))\n",
    "                print(f\"âœ… Group {group_idx} - keeping all files (no deletion)\")\n",
    "                break\n",
    "            elif user_input in ['d', 'delete']:\n",
    "                deleted_groups.append((group_idx, group))\n",
    "                print(f\"ðŸ—‘ï¸ Group {group_idx} - will delete duplicates (keep first file)\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter 'k/keep' to keep all files or 'd/delete' to delete duplicates only\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"REVIEW SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total groups reviewed: {len(duplicate_groups)}\")\n",
    "    print(f\"Groups keeping all files: {len(kept_groups)}\")\n",
    "    print(f\"Groups deleting duplicates: {len(deleted_groups)}\")\n",
    "    \n",
    "    return kept_groups, deleted_groups\n",
    "\n",
    "print(\"Display function with keep/delete option defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution - Configure and run duplicate detection\n",
    "# Using your specified folder path\n",
    "IMAGE_FOLDER_PATH = r\"path\"\n",
    "\n",
    "# Adjust similarity threshold (lower = more strict, higher = more lenient)\n",
    "# Recommended: 5-10 for exact/near-exact duplicates, 10-15 for similar images\n",
    "SIMILARITY_THRESHOLD = 5\n",
    "\n",
    "print(f\"Analyzing images in: {IMAGE_FOLDER_PATH}\")\n",
    "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(\"Starting duplicate detection...\")\n",
    "\n",
    "# Run the duplicate detection\n",
    "start_time = time.time()\n",
    "duplicate_groups = find_duplicate_images(IMAGE_FOLDER_PATH, SIMILARITY_THRESHOLD)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nDuplicate detection completed in {total_time:.2f} seconds\")\n",
    "print(f\"Time complexity: O(n log n) where n = number of images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY AND MANUAL REVIEW OF DUPLICATES\n",
    "# Display results with manual review and keep/delete option\n",
    "\n",
    "print(\"ðŸ“‹ MANUAL REVIEW OF DUPLICATE GROUPS\")\n",
    "print(\"Review each group:\")\n",
    "print(\"- Keep: Keep ALL files in the group (no deletion)\")\n",
    "print(\"- Delete: Keep first file, delete the rest (duplicates)\")\n",
    "print()\n",
    "\n",
    "kept_groups, deleted_groups = display_duplicate_groups_with_keep_delete(duplicate_groups)\n",
    "\n",
    "# Process groups based on user decisions\n",
    "files_to_delete = []\n",
    "files_to_keep = []\n",
    "\n",
    "# For kept groups - keep ALL files (no deletion)\n",
    "for group_idx, group in kept_groups:\n",
    "    files_to_keep.extend(group)  # Keep all files in the group\n",
    "\n",
    "# For deleted groups - keep first file, delete the rest (duplicates)\n",
    "for group_idx, group in deleted_groups:\n",
    "    files_to_keep.append(group[0])  # Keep the first file\n",
    "    files_to_delete.extend(group[1:])  # Delete the duplicates\n",
    "\n",
    "# Summary statistics\n",
    "total_kept_groups = len(kept_groups)\n",
    "total_deleted_groups = len(deleted_groups)\n",
    "total_files_to_delete = len(files_to_delete)\n",
    "total_files_to_keep = len(files_to_keep)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SUMMARY STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Groups keeping all files: {total_kept_groups}\")\n",
    "print(f\"Groups deleting duplicates: {total_deleted_groups}\")\n",
    "print(f\"Total files to keep: {total_files_to_keep}\")\n",
    "print(f\"Total files to delete: {total_files_to_delete}\")\n",
    "\n",
    "# Save comprehensive report\n",
    "if kept_groups or deleted_groups:\n",
    "    # Save main report\n",
    "    with open('duplicate_report.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"DUPLICATE IMAGE DETECTION REPORT\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        if kept_groups:\n",
    "            f.write(\"KEPT GROUPS (keeping ALL files, no deletion):\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            for original_group_idx, group in kept_groups:\n",
    "                f.write(f\"Group {original_group_idx} ({len(group)} files - all kept):\\n\")\n",
    "                for i, image_path in enumerate(group):\n",
    "                    f.write(f\"  KEEP: {image_path}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        if deleted_groups:\n",
    "            f.write(\"DUPLICATE DELETION GROUPS (keeping first, deleting rest):\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            for original_group_idx, group in deleted_groups:\n",
    "                f.write(f\"Group {original_group_idx} ({len(group)} files - keeping first, deleting duplicates):\\n\")\n",
    "                f.write(f\"  KEEP: {group[0]}\\n\")\n",
    "                for i, image_path in enumerate(group[1:]):\n",
    "                    f.write(f\"  DELETE: {image_path}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        f.write(f\"SUMMARY:\\n\")\n",
    "        f.write(f\"Groups keeping all files: {total_kept_groups}\\n\")\n",
    "        f.write(f\"Groups deleting duplicates: {total_deleted_groups}\\n\")\n",
    "        f.write(f\"Total files to keep: {total_files_to_keep}\\n\")\n",
    "        f.write(f\"Total files to delete: {total_files_to_delete}\\n\")\n",
    "    \n",
    "    # Save separate deletion list\n",
    "    with open('files_to_delete.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"FILES TO DELETE\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"Total files marked for deletion: {total_files_to_delete}\\n\\n\")\n",
    "        \n",
    "        for file_path in files_to_delete:\n",
    "            f.write(f\"{file_path}\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… Complete report saved to 'duplicate_report.txt'\")\n",
    "    print(f\"âœ… Deletion list saved to 'files_to_delete.txt'\")\n",
    "    print(\"Ready for deletion process!\")\n",
    "else:\n",
    "    print(\"\\nâœ… No duplicate groups found - your image collection is clean!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETION PROCESS\n",
    "# Delete files listed in files_to_delete.txt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "def delete_files_from_list(file_list_path):\n",
    "    \"\"\"\n",
    "    Delete files listed in the specified text file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_list_path):\n",
    "        print(f\"âŒ File list not found: {file_list_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“‹ Reading deletion list from: {file_list_path}\")\n",
    "    \n",
    "    # Read the files to delete\n",
    "    files_to_delete = []\n",
    "    with open(file_list_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Skip header lines and empty lines\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('FILES TO DELETE') and not line.startswith('=') and not line.startswith('Total files'):\n",
    "                if os.path.exists(line):\n",
    "                    files_to_delete.append(line)\n",
    "    \n",
    "    if not files_to_delete:\n",
    "        print(\"âœ… No files to delete found or all files already deleted.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files_to_delete)} files to delete\")\n",
    "    \n",
    "    # Confirm deletion\n",
    "    print(f\"\\nâš ï¸  WARNING: About to delete {len(files_to_delete)} duplicate files!\")\n",
    "    print(\"This action cannot be undone.\")\n",
    "    \n",
    "    while True:\n",
    "        confirm = input(\"Do you want to proceed with deletion? (yes/no): \").strip().lower()\n",
    "        if confirm in ['yes', 'y']:\n",
    "            break\n",
    "        elif confirm in ['no', 'n']:\n",
    "            print(\"âŒ Deletion cancelled by user.\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Please enter 'yes' or 'no'\")\n",
    "    \n",
    "    # Perform deletion\n",
    "    deleted_count = 0\n",
    "    failed_count = 0\n",
    "    deleted_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    print(f\"\\nðŸ—‘ï¸  Starting deletion process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_delete, 1):\n",
    "        try:\n",
    "            print(f\"Deleting {i}/{len(files_to_delete)}: {os.path.basename(file_path)}\")\n",
    "            os.remove(file_path)\n",
    "            deleted_files.append(file_path)\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to delete {file_path}: {e}\")\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            failed_count += 1\n",
    "    \n",
    "    deletion_time = time.time() - start_time\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DELETION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"âœ… Successfully deleted: {deleted_count} files\")\n",
    "    print(f\"âŒ Failed to delete: {failed_count} files\")\n",
    "    print(f\"â±ï¸  Time taken: {deletion_time:.2f} seconds\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\nâŒ Failed deletions:\")\n",
    "        for file_path, error in failed_files:\n",
    "            print(f\"  {file_path} - {error}\")\n",
    "    \n",
    "    # Save deletion log\n",
    "    log_filename = f\"deletion_log_{int(time.time())}.txt\"\n",
    "    with open(log_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"DELETION LOG\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Deletion completed on: {time.ctime()}\\n\")\n",
    "        f.write(f\"Total files processed: {len(files_to_delete)}\\n\")\n",
    "        f.write(f\"Successfully deleted: {deleted_count}\\n\")\n",
    "        f.write(f\"Failed to delete: {failed_count}\\n\")\n",
    "        f.write(f\"Time taken: {deletion_time:.2f} seconds\\n\\n\")\n",
    "        \n",
    "        if deleted_files:\n",
    "            f.write(\"SUCCESSFULLY DELETED FILES:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for file_path in deleted_files:\n",
    "                f.write(f\"âœ… {file_path}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if failed_files:\n",
    "            f.write(\"FAILED DELETIONS:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for file_path, error in failed_files:\n",
    "                f.write(f\"âŒ {file_path} - {error}\\n\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Deletion log saved to: {log_filename}\")\n",
    "    \n",
    "    return deleted_count, failed_count\n",
    "\n",
    "# Execute the deletion\n",
    "if os.path.exists('files_to_delete.txt'):\n",
    "    print(\"ðŸš€ Starting file deletion process...\")\n",
    "    deleted, failed = delete_files_from_list('files_to_delete.txt')\n",
    "    \n",
    "    if deleted > 0:\n",
    "        print(f\"\\nðŸŽ‰ Deletion process completed!\")\n",
    "        print(f\"   Space freed up by deleting {deleted} duplicate files\")\n",
    "        \n",
    "        # Optionally clean up the deletion list file\n",
    "        cleanup = input(\"\\nDo you want to delete the 'files_to_delete.txt' file? (yes/no): \").strip().lower()\n",
    "        if cleanup in ['yes', 'y']:\n",
    "            try:\n",
    "                os.remove('files_to_delete.txt')\n",
    "                print(\"âœ… Cleanup completed - 'files_to_delete.txt' deleted\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to delete 'files_to_delete.txt': {e}\")\n",
    "else:\n",
    "    print(\"âŒ No 'files_to_delete.txt' file found. Run the duplicate detection first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICATION: Re-scan for remaining duplicates after deletion\n",
    "# This will show if there are any remaining duplicates without user interaction\n",
    "\n",
    "print(\"ðŸ” VERIFICATION SCAN: Checking for remaining duplicates after deletion...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Re-run duplicate detection on the same folder\n",
    "verification_start_time = time.time()\n",
    "remaining_duplicates = find_duplicate_images(IMAGE_FOLDER_PATH, SIMILARITY_THRESHOLD)\n",
    "verification_time = time.time() - verification_start_time\n",
    "\n",
    "print(f\"\\nVerification scan completed in {verification_time:.2f} seconds\")\n",
    "\n",
    "def display_remaining_duplicates_readonly(duplicate_groups):\n",
    "    \"\"\"\n",
    "    Display remaining duplicate image groups without user interaction\n",
    "    This is for verification purposes only\n",
    "    \"\"\"\n",
    "    if not duplicate_groups:\n",
    "        print(\"\\nðŸŽ‰ SUCCESS: No duplicate images found!\")\n",
    "        print(\"âœ… Your image collection is now clean of duplicates.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâš ï¸  Found {len(duplicate_groups)} groups of remaining duplicate images:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_remaining_files = sum(len(group) for group in duplicate_groups)\n",
    "    print(f\"Total remaining duplicate files: {total_remaining_files}\")\n",
    "    print(\"(This is for verification - no action will be taken)\")\n",
    "    print()\n",
    "    \n",
    "    for group_idx, group in enumerate(duplicate_groups, 1):\n",
    "        print(f\"\\nRemaining Group {group_idx}: {len(group)} duplicate images\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Display file paths\n",
    "        for i, image_path in enumerate(group):\n",
    "            print(f\"{i+1}. {os.path.basename(image_path)}\")\n",
    "            print(f\"   {image_path}\")\n",
    "        \n",
    "        # Display images in a grid (limit to 4 images for display)\n",
    "        num_images_to_show = min(len(group), 4)\n",
    "        fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 4))\n",
    "        \n",
    "        # Handle single image case\n",
    "        if num_images_to_show == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        if len(group) > 4:\n",
    "            print(f\"   (Showing first 4 out of {len(group)} images)\")\n",
    "            \n",
    "        for i, image_path in enumerate(group[:num_images_to_show]):\n",
    "            try:\n",
    "                img = Image.open(image_path)\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Image {i+1}\", fontsize=10)\n",
    "                axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {image_path}: {e}\")\n",
    "                # Show error text in the subplot\n",
    "                axes[i].text(0.5, 0.5, f\"Error loading\\\\nimage {i+1}\", \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "# Display results\n",
    "display_remaining_duplicates_readonly(remaining_duplicates)\n",
    "\n",
    "# Summary statistics\n",
    "if remaining_duplicates:\n",
    "    total_remaining_duplicates = sum(len(group) for group in remaining_duplicates)\n",
    "    print(f\"\\nðŸ“Š VERIFICATION SUMMARY:\")\n",
    "    print(f\"   â€¢ Remaining duplicate groups: {len(remaining_duplicates)}\")\n",
    "    print(f\"   â€¢ Total remaining duplicate files: {total_remaining_duplicates}\")\n",
    "    print(f\"   â€¢ You may want to review these manually or adjust the similarity threshold\")\n",
    "else:\n",
    "    print(f\"\\nðŸŽ‰ VERIFICATION COMPLETE:\")\n",
    "    print(f\"   â€¢ No remaining duplicates found\")\n",
    "    print(f\"   â€¢ Deletion process was successful\")\n",
    "    print(f\"   â€¢ Your image collection is now clean!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
